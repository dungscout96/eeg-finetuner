foundation_model:
  model_name: "labram"
  n_times: 256
  n_chans: 129
  n_outputs: 128
  embed_dim: 128
task:
  task_type: "classification"
  num_classes: 3
  decoder_type: "linear"
data:
  dataset: "ds005514"
  batch_size: 16
training:
  freeze_backbone: true
  learning_rate: 0.001
  max_epochs: 2
